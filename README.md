# üî¨ Multi-Modal AI Research Assistant

A sophisticated RAG (Retrieval-Augmented Generation) application that allows users to ask complex questions about scientific papers by understanding their text, images, and tables.

---

## üöÄ Live Demo

**[Link to your deployed Streamlit App will go here]**

---

## üåü Overview

This project is an advanced AI-powered research assistant designed to intelligently analyze and answer questions from complex, multi-modal documents like scientific papers. Unlike basic RAG systems that only handle text, this application can interpret and synthesize information from text paragraphs, structured data in tables, and visual information in charts and diagrams.

The goal is to provide a powerful tool for researchers, students, and professionals to quickly extract and understand key information from dense research documents, significantly speeding up the literature review process.

---

## ‚ú® Key Features

* **Multi-Modal RAG Pipeline:** Ingests PDF documents and processes three types of content:
    * **Text:** Parses and chunks unstructured text.
    * **Images:** Extracts and embeds images, charts, and diagrams.
    * **Tables:** Extracts and summarizes structured data from tables.
* **Hybrid Retrieval System:** Utilizes a hybrid search mechanism that queries three separate vector stores (for text, images, and table summaries) to retrieve the most relevant context for a user's question.
* **Vision-Capable Generation:** Leverages a powerful multi-modal LLM (Google's Gemini Pro) to generate answers based on a combined context of text, images, and tables.
* **Professional & Interactive UI:** A clean, user-friendly interface built with Streamlit, featuring a custom theme, organized tabs for content, and session management.
* **Robust & Resilient:** Incorporates error handling for PDF processing, rate-limiting protection for API calls, and defensive coding practices.

---

## üîß Technology Stack

* **Backend & Logic:** Python
* **Web Framework:** Streamlit
* **LLM:** Google Gemini Pro
* **Embedding Models:** `all-MiniLM-L6-v2` (for text), `clip-ViT-B-32` (for images)
* **Vector Database:** ChromaDB (in-memory)
* **Data Processing:** LangChain (for text splitting), PyMuPDF (for PDF parsing), Pandas (for table handling), Pillow (for images)
* **API & Environment:** `google-generativai`, `python-dotenv`

---

## üèõÔ∏è Architecture

*[```mermaid
graph TD
    subgraph "Data Processing / Ingestion"
        A[User Uploads PDF] --> B{Process PDF};
        B --> C[Extract Text];
        B --> D[Extract Images];
        B --> E[Extract Tables];

        C --> F[Chunk Text];
        F --> G[Embed Text Chunks <br>(all-MiniLM-L6-v2)];
        G --> H1[(Text Vector Store <br> ChromaDB)];

        D --> I[Embed Images <br>(CLIP)];
        I --> H2[(Image Vector Store <br> ChromaDB)];

        E --> J[Summarize Tables <br>(Gemini)];
        J --> K[Embed Summaries <br>(all-MiniLM-L6-v2)];
        K --> H3[(Table Vector Store <br> ChromaDB)];
    end

    subgraph "Query / Generation"
        L[User Asks Question] --> M{Hybrid Retrieval};
        H1 --> M;
        H2 --> M;
        H3 --> M;

        M --> N[Combine Text, Images, & Tables into Context];
        N --> O{Multi-Modal LLM <br>(Gemini Pro)};
        L --> O;
        O --> P[Display Answer to User];
    end

    style A fill:#cde4ff
    style L fill:#cde4ff
    style P fill:#cde4ff
```]*

---

## üéØ Performance Showcase

To demonstrate the application's capabilities, here are some sample questions asked about the **"MobileNets" paper** ([link](https://arxiv.org/pdf/1704.04861)) and the high-quality answers generated by the model.

#### Q1 (Text-Based)
**Question:** "What are the two simple global hyper-parameters that MobileNets introduce?"
**Answer:** The two simple global hyper-parameters that MobileNets introduce are a width multiplier and a resolution multiplier.

#### Q2 (Table-Based)
**Question:** "According to Table 3, how many millions of parameters does the '1.0 MobileNet-224' model have?"
**Answer:** The '1.0 MobileNet-224' model has 4.2 Million parameters.

#### Q3 (Image-Based)
**Question:** "Based on the diagram in Figure 2, what are the two separate operations that make up a depthwise separable convolution?"
**Answer:** The two separate operations are depthwise convolution and pointwise convolution.

---

## üõ†Ô∏è Setup and Installation

To run this project locally, please follow these steps:

1.  **Clone the repository:**
    ```bash
    git clone [Your Repository URL]
    cd [Your Repository Name]
    ```

2.  **Create and activate a virtual environment:**
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # On Windows, use: .\.venv\Scripts\activate
    ```

3.  **Install the required dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set up your environment variables:**
    * Create a file named `.env` in the root directory.
    * Add your Google API key to the file:
        ```
        GOOGLE_API_KEY='YOUR_API_KEY_HERE'
        ```

5.  **Run the Streamlit application:**
    ```bash
    streamlit run main.py
    ```

---

## üîÆ Future Work

* Implement a caching mechanism for table summaries to reduce API calls.
* Integrate a formal evaluation framework like Ragas to continuously monitor performance.
* Allow for processing multiple documents and querying across them.